---
output: 
  github_document: 
    pandoc_args: --webtex=https://latex.codecogs.com/svg.latex?
bibliography: README.bib
nocite: | 
  @Mahjani20
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  cache.path = "cache/README-",
  out.width = "100%", error = FALSE)
options(digits = 3)
```

# survTMB
[![Build Status on Travis](https://travis-ci.org/boennecd/survTMB.svg?branch=master,osx)](https://travis-ci.org/boennecd/survTMB)
<!-- [![](https://www.r-pkg.org/badges/version/survTMB)](https://www.r-pkg.org/badges/version/survTMB) -->
<!-- [![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/survTMB)](https://cran.r-project.org/package=survTMB) -->

This package contains methods to estimated mixed generalized survival 
models [@Liu16;@Liu17]. All methods use automatic differentiation using 
the CppAD library [@Bell19] through [the TMB package](https://github.com/kaskr/adcomp) 
[@Kristensen16]. The estimation methods are 

 - a Laplace approximation using [TMB](https://github.com/kaskr/adcomp). 
 - Gaussian variational approximation (GVA) similar to the method shown 
   by @Ormerod12. 
 - Skew-normal variational approximation (SNVA) similar to the method shown
   by @Ormerod11.
   
The [example](#example) section shows an example of how to use the package
with different methods. The [benchmark](#benchmark) section shows a 
comparison of the computation time of the methods.

Joint marker and survival models are also available in the package. We show 
an example of estimating a joint model in the 
[joint models](#joint-models) section.

## Installation
The package can be installed from Github by calling:

```{r how_to_install, eval = FALSE}
library(remotes)
install_github("boennecd/psqn")    # we need the psqn package
install_github("boennecd/survTMB")
```

## Example

We estimate a GSM a below with the proportional odds (PO) link function 
using both a Laplace approximation, a GVA, and a SNVA. 
First, we define a function to perform the estimation.

```{r fit_example, cache = 1, fig.width = 5, fig.height = 3.67}
# assign variable with data 
dat <- coxme::eortc

# assign function to estimate the model
library(survTMB)
library(survival)
fit_model <- function(link, n_threads = 2L, method = "Laplace", 
                      param_type = "DP", dense_hess = FALSE, 
                      sparse_hess = FALSE, do_fit = TRUE, 
                      do_free = FALSE)
  eval(bquote({
    adfun <- make_mgsm_ADFun(
      Surv(y, uncens) ~ trt, cluster = as.factor(center), 
      Z = ~ trt, df = 3L, data = dat, link = .(link), do_setup = .(method), 
      n_threads = .(n_threads), param_type = .(param_type), n_nodes = 15L, 
      dense_hess = .(dense_hess), sparse_hess = .(sparse_hess))
    fit <- if(.(do_fit))
      fit_mgsm(adfun, method = .(method)) else NULL
    if(.(do_free)){
      free_laplace(adfun)
      clear_cppad_mem(.(n_threads), keep_work_space = TRUE)
    }
    list(fit = fit, fun = adfun)
  }), parent.frame())

# # estimate the model using different methods. Start w/ Laplace
# (lap_ph <- fit_model("PO"))$fit

# w/ GVA
(gva_fit <- fit_model("PO", method = "GVA"))$fit

# w/ SNVA
(snva_fit <- fit_model("PO", method = "SNVA", param_type = "DP"))$fit
```

### Computing the Hessian

The Hessian using a variational approximation (VA) can be computed as both 
a dense matrix and as a sparse matrix. We show an example below where
we compare the two approaches. 

```{r load_micro}
library(microbenchmark) # needed for benchmarking
```

```{r comp_hess, cache = 1, dependson = "fit_example"}
# fit model w/ GVA
fit <- fit_model("PO", method = "GVA", dense_hess = TRUE, 
                 sparse_hess = TRUE)

# compute dense Hessian
par <- with(fit$fit, c(params, va_params))
dense_hess <- fit$fun$gva$he(par)
num_hess <- numDeriv::jacobian(fit$fun$gva$gr, par)
all.equal(dense_hess, num_hess, tolerance = 1e-5)

# has many zeros (i.e. it is sparse)
mean(abs(dense_hess) > .Machine$double.eps) # fraction of non-zeros

# plot non-zero entries (black block's are non-zero; ignore upper triangle)
par(mar = c(1, 1, 1, 1))
is_non_zero <- t(abs(dense_hess) > .Machine$double.eps)
is_non_zero[upper.tri(is_non_zero)] <- FALSE
image(is_non_zero, xaxt = "n", yaxt = "n", 
      col = gray.colors(2, 1, 0))

# compute sparse Hessian
sparse_hess <- fit$fun$gva$he_sp(par)

# they are identical 
stopifnot(isTRUE(
  all.equal(as.matrix(sparse_hess), dense_hess, check.attributes = FALSE)))

# compare storage cost
as.numeric(object.size(dense_hess) / object.size(sparse_hess))

# we usually want the first part the inverse negative Hessian for the model 
# parameters. This can be computed as follows
library(Matrix)
n_vars <- length(fit$fit$params)
naiv_vcov <- function(hess)
  solve(hess)[1:n_vars, 1:n_vars]
alte_vcov <- function(hess){
  idx <- 1:n_vars
  A <- hess[ idx,  idx]
  C <- hess[-idx,  idx]
  D <- hess[-idx, -idx]
  solve(A - crossprod(C, solve(D, C)))
}

# these are the asymptotic standard deviations
structure(sqrt(diag(alte_vcov(dense_hess))), names = names(fit$fit$params))

# check output is the same
stopifnot(
  isTRUE(all.equal(naiv_vcov(dense_hess), alte_vcov(dense_hess))),
  isTRUE(all.equal(naiv_vcov(dense_hess), as.matrix(alte_vcov(sparse_hess)), 
                   check.attributes = FALSE)),
  isTRUE(all.equal(naiv_vcov(dense_hess), as.matrix(naiv_vcov(sparse_hess)), 
                   check.attributes = FALSE)))

# compare computation time
microbenchmark(
  `Compute dense Hessian`               = fit$fun$gva$he(par), 
  `Compute sparse Hessian`              = fit$fun$gva$he_sp(par), 
  `Invert dense Hessian (naive)`        = naiv_vcov(dense_hess), 
  `Invert sparse Hessian (naive)`       = naiv_vcov(sparse_hess),
  `Invert dense Hessian (alternative)`  = alte_vcov(dense_hess), 
  `Invert sparse Hessian (alternative)` = alte_vcov(sparse_hess),
  times = 10)
```

The sparse matrix only becomes more favorable for larger data sets
(that is, in terms of the number of clusters). However, 
[recording](https://www.coin-or.org/CppAD/Doc/independent.htm) takes some 
time and requires additional memory. We illustrate the additional time 
below. 

```{r extra_time, cache = 1, dependson = "fit_example"}
microbenchmark(
  `W/o Hessians     ` = fit_model("PO", method = "GVA", do_fit = FALSE), 
  `W/ dense Hessian ` = fit_model("PO", method = "GVA", do_fit = FALSE, 
                                  dense_hess = TRUE), 
  `W/ sparse Hessian` = fit_model("PO", method = "GVA", do_fit = FALSE, 
                                  sparse_hess = TRUE), 
  times = 10)
```

### Approximation of the Conditional Distribution
The variational parameters provide an approximation of the conditional 
distribution given the data and parameters or the posterior in a Bayesian 
view. As an example, we can look at the multivariate normal distribution 
approximation which is made by the GVA for the first group below.

```{r GVA_look_approx}
va_params <- gva_fit$fit$va_params
is_this_group <- which(grepl("^g1:", names(va_params)))
n_random_effects <- 2L

# conditional mean of random effects
va_params[is_this_group][seq_len(n_random_effects)]

# conditional covariance matrix of random effects
theta_to_cov(va_params[is_this_group][-seq_len(n_random_effects)])
```

We can compare this with the  multivariate skew-normal distribution 
approximation from the SNVA.

```{r snva_look_approx}
va_params <- snva_fit$fit$va_params
is_this_group <- which(grepl("^g1:", names(va_params)))
n_random_effects <- 2L

xi <- va_params[is_this_group][seq_len(n_random_effects)]
Psi <- head(tail(va_params[is_this_group], -n_random_effects), 
            -n_random_effects)
Psi <- theta_to_cov(Psi)
alpha <- tail(va_params[is_this_group], n_random_effects)

# conditional mean, covariance matrix, and Pearson's moment coefficient of 
# skewness
dp_to_cp(xi = xi, Psi = Psi, alpha = alpha)
```

from the default values possibly because the lower bound is quite flat in 
these parameters in this area.

```{r snva_check_skew}
skews <- sapply(1:37, function(id){
  va_params <- snva_fit$fit$va_params
  is_this_group <- which(grepl(paste0("^g", id, ":"), names(va_params)))
  
  xi <- va_params[is_this_group][seq_len(n_random_effects)]
  Psi <- head(tail(va_params[is_this_group], -n_random_effects), 
              -n_random_effects)
  Psi <- theta_to_cov(Psi)
  alpha <- tail(va_params[is_this_group], n_random_effects)
  dp_to_cp(xi = xi, Psi = Psi, alpha = alpha)$gamma
})

apply(skews, 1L, quantile, probs = seq(0, 1, by = .25))
```

Again, the skewness parameter have not moved much from the defaults.

### Other link functions
We estimate the same model below with other link functions.

```{r other_link, cache = 1, dependson = "fit_example"}
# ######
# # w/ Laplace
# fit_model("PH"    , do_free = TRUE)$fit
# fit_model("PO"    , do_free = TRUE)$fit
# fit_model("probit", do_free = TRUE)$fit

######
# w/ GVA
fit_model("PH"    , method = "GVA")$fit
fit_model("PO"    , method = "GVA")$fit
fit_model("probit", method = "GVA")$fit

######
# w/ SNVA (DP: direct parameterization)
fit_model("PH"    , method = "SNVA", param_type = "DP")$fit
fit_model("PO"    , method = "SNVA", param_type = "DP")$fit
fit_model("probit", method = "SNVA", param_type = "DP")$fit

######
# w/ SNVA (CP: centralized parameterization)
fit_model("PH"    , method = "SNVA", param_type = "CP_trans")$fit
fit_model("PO"    , method = "SNVA", param_type = "CP_trans")$fit
fit_model("probit", method = "SNVA", param_type = "CP_trans")$fit
```

## Benchmark
We provide a benchmark of the estimation methods used in section 
[example](#example) below.

```{r clear_mem_comp_time}
clear_cppad_mem(4L)
```

```{r comp_time, cache = 1, dependson = "fit_example"}
for(mth in c("GVA")){
# for(mth in c("Laplace", "GVA")){
  msg <- sprintf("Method: %s", mth)
  cat(sprintf("\n%s\n%s\n", msg, 
              paste0(rep("-", nchar(msg)), collapse = "")))
  print(microbenchmark(
    `PH         ` = fit_model("PH"    , 1L, mth, do_free = TRUE),
    `PH     (2L)` = fit_model("PH"    , 2L, mth, do_free = TRUE),
    `PH     (4L)` = fit_model("PH"    , 4L, mth, do_free = TRUE),
    
    `PO         ` = fit_model("PO"    , 1L, mth, do_free = TRUE),
    `PO     (2L)` = fit_model("PO"    , 2L, mth, do_free = TRUE),
    `PO     (4L)` = fit_model("PO"    , 4L, mth, do_free = TRUE), 
    
    `probit     ` = fit_model("probit", 1L, mth, do_free = TRUE),
    `probit (2L)` = fit_model("probit", 2L, mth, do_free = TRUE),
    `probit (4L)` = fit_model("probit", 4L, mth, do_free = TRUE),
    times = 5))
}
```

```{r SNVA_comp_time, cache = 1, dependson = "fit_example"}
for(param_type in c("DP", "CP_trans")){
  mth <- "SNVA"
  msg <- sprintf("Method: %s (%s)", mth, param_type)
  cat(sprintf("\n%s\n%s\n", msg, 
              paste0(rep("-", nchar(msg)), collapse = "")))
  print(suppressWarnings(microbenchmark(
    `PH         ` = fit_model("PH"    , 1L, mth, param_type = param_type),
    `PH     (2L)` = fit_model("PH"    , 2L, mth, param_type = param_type),
    `PH     (4L)` = fit_model("PH"    , 4L, mth, param_type = param_type),
    
    `PO         ` = fit_model("PO"    , 1L, mth, param_type = param_type),
    `PO     (2L)` = fit_model("PO"    , 2L, mth, param_type = param_type),
    `PO     (4L)` = fit_model("PO"    , 4L, mth, param_type = param_type), 
    
    `probit     ` = fit_model("probit", 1L, mth, param_type = param_type),
    `probit (2L)` = fit_model("probit", 2L, mth, param_type = param_type),
    `probit (4L)` = fit_model("probit", 4L, mth, param_type = param_type),
    times = 5)))
}
```

### Using the psqn Interface
Another option is to use [the psqn package](https://github.com/boennecd/psqn)
through this package. This can be done as follows

```{r mgsm_use_psqn, cache = 1}
# get the object needed to perform the estimation 
psqn_obj <- make_mgsm_psqn_obj(
  formula = Surv(y, uncens) ~ trt, data = dat, 
  df = 3L, Z = ~trt, cluster = as.factor(center), do_setup = "SNVA", 
  n_nodes = 15L, link = "PO", n_threads = 2L)

# perform the estimation
psqn_opt <- optim_mgsm_psqn(psqn_obj)

# check that the function value is the same
all.equal(psqn_opt$value, snva_fit$fit$optim$value)
```

It is a bit slower in this case as shown below but can be faster when there 
are more clusters.

```{r mgsm_time_psqn, cache = 1}
microbenchmark(`Using psqn` = { 
  psqn_obj <- make_mgsm_psqn_obj(
    formula = Surv(y, uncens) ~ trt, data = dat, 
    df = 3L, Z = ~trt, cluster = as.factor(center), do_setup = "SNVA", 
    n_nodes = 15L, link = "PO", n_threads = 2L)
  optim_mgsm_psqn(psqn_obj)
}, times = 5)
```

```{r rep_rep_clear_mem_comp_time, echo = FALSE, results = "hide"}
rm(list = ls())
gc()
clear_cppad_mem(4L)
```

## Joint Models
We will use one of the test data sets in the 
[inst/test-data](inst/test-data) directory. The data is 
generated with the 
[inst/test-data/gen-test-data.R](inst/test-data/gen-test-data.R) file which 
is available on Github. The file uses the 
[SimSurvNMarker](https://github.com/boennecd/SimSurvNMarker) package 
to simulate a data set. The model is simulated from 

<!-- $$\begin{align*} -->
<!-- \vec Y_{ij} \mid \vec U_i = \vec u_i -->
<!--   &\sim N^{(r)}(\vec \mu_i(s_{ij}, \vec u_i), \Sigma) -->
<!--   \\ -->
<!-- \vec\mu(s, \vec u) &= -->
<!--   \Gamma^\top \vec x_i + B^\top\vec g(s) + U^\top\vec m(s) -->
<!--   \\ -->
<!-- &= \left(I \otimes \vec x_i^\top\right)\text{vec}\Gamma -->
<!--      + \left(I \otimes \vec g(s)^\top\right)\text{vec} B -->
<!--      + \left(I \otimes \vec m(s)^\top\right) \vec u -->
<!--   \\ -->
<!-- \vec U_i &\sim N^{(K)}(\vec 0, \Psi) -->
<!--   \\ -->
<!-- h(t\mid \vec u) &= \exp\left( -->
<!--   \vec\omega^\top\vec b(t) + -->
<!--   \vec z_i^\top\vec\delta + -->
<!--   \vec\alpha^\top\vec\mu(t, \vec u) -->
<!--   \right) -->
<!--   \\ -->
<!-- &= \exp\Bigg( -->
<!--   \vec\omega^\top\vec b(t) + -->
<!--   \vec z_i^\top\vec\delta -->
<!--   + \vec 1^\top\left( -->
<!--   \text{diag}(\vec \alpha) \otimes \vec x_i^\top\right)\text{vec}\Gamma -->
<!--   + \vec 1^\top\left( -->
<!--   \text{diag}(\vec \alpha) \otimes \vec g(t)^\top\right)\text{vec} B \\ -->
<!-- &\hspace{50pt}+ \vec 1^\top\left( -->
<!--   \text{diag}(\vec \alpha) \otimes \vec m(t)^\top\right)\vec u -->
<!--   \Bigg) -->
<!-- \end{align*}$$ -->

$$\begin{align*}  \vec Y_{ij} \mid \vec U_i = \vec u_i    &\sim N^{(r)}(\vec \mu_i(s_{ij}, \vec u_i), \Sigma)    \\  \vec\mu(s, \vec u) &=    \Gamma^\top \vec x_i + B^\top\vec g(s) + U^\top\vec m(s)    \\  &= \left(I \otimes \vec x_i^\top\right)\text{vec}\Gamma       + \left(I \otimes \vec g(s)^\top\right)\text{vec} B       + \left(I \otimes \vec m(s)^\top\right) \vec u    \\  \vec U_i &\sim N^{(K)}(\vec 0, \Psi)    \\  h(t\mid \vec u) &= \exp\left(    \vec\omega^\top\vec b(t) +    \vec z_i^\top\vec\delta +    \vec\alpha^\top\vec\mu(t, \vec u)    \right)    \\  &= \exp\Bigg(    \vec\omega^\top\vec b(t) +    \vec z_i^\top\vec\delta    + \vec 1^\top\left(    \text{diag}(\vec \alpha) \otimes \vec x_i^\top\right)\text{vec}\Gamma    + \vec 1^\top\left(    \text{diag}(\vec \alpha) \otimes \vec g(t)^\top\right)\text{vec} B \\  &\hspace{50pt}+ \vec 1^\top\left(    \text{diag}(\vec \alpha) \otimes \vec m(t)^\top\right)\vec u    \Bigg)  \end{align*}$$

where $\vec Y_{ij}\in\mathbb R^{n_y}$ is individual $i$'s 
$j$th observed marker at time $s_{ij}$, 
$U_i\in\mathbb R^K$ is individual $i$'s random effect, and 
$h$ is the instantaneous hazard rate for the time-to-event outcome. 
$\vec\alpha$ is the so-called
association parameter. It shows the strength of the relation between the 
latent mean function, $\vec\mu(t,\vec u)$, and the log of the 
instantaneous rate, $h(t\mid \vec u)$. $\vec m(t)$, $\vec g(t)$ 
and $\vec b(t)$ are 
basis expansions of time. As an example, these can be a polynomial, 
a B-spline, or a natural cubic spline. The expansion for the baseline
hazard, $\vec b(t)$, is typically made on $\log t$ instead of $t$. 
One reason is that the model reduces to a Weibull distribution
when a first polynomial is used and $\vec\alpha = \vec 0$. $\vec x_i$ and 
$\vec z_i$ are individual specific known covariates.

We start by loading the simulated data set.

```{r read_joint}
dat <- readRDS(file.path("inst", "test-data", "large-joint-all.RDS"))

# the marker data
m_data <- dat$marker_data
head(m_data, 10)

# the survival data
s_data <- dat$survival_data
head(s_data, 10)
```

There is 

```{r joint_show_n_ids}
length(unique(s_data$id))
length(unique(s_data$id)) == NROW(s_data) # one row per id
```

individuals who each has an average of 

```{r joint_avg_markers}
NROW(m_data) / length(unique(s_data$id))
```

observed markers. The data is simulated. Thus, we know the true parameters. 
These are

```{r joint_show_true_params}
dat$params[c("gamma", "B", "Psi", "omega", "delta", "alpha", "sigma")]
```

We start by constructing the objective function in order to estimate 
the model. 

```{r joint_get_ptr, cache = 1}
system.time(
  out <- make_joint_ADFun(
    sformula =  Surv(left_trunc, y, event) ~ Z1 + Z2, 
    mformula = cbind(Y1, Y2) ~ X1, 
    id_var = id, time_var = obs_time, 
    sdata = s_data, mdata = m_data, m_coefs = dat$params$m_attr$knots,
    s_coefs = dat$params$b_attr$knots, g_coefs = dat$params$g_attr$knots, 
    n_nodes = 30L, n_threads = 6L))
```

Next, we fit the model using the default optimization function.

```{r joint_fit, cache = 1, dependson = "joint_get_ptr"}
library(lbfgs)
system.time(
  opt_out <- lbfgs(out$fn, out$gr, out$par, max_iterations = 25000L, 
                   past = 100L, delta = sqrt(.Machine$double.eps), 
                   invisible = 1))
```

The estimated lower bound of the log marginal likelihood at the optimum is
shown below.

<!-- with(environment(out$fn), c(mark$ll, sr_dat$ll, mark$ll + sr_dat$ll)) -->

```{r joint_show_lb}
-opt_out$value
```

Further, we can compare the estimated model parameters with the true 
model parameters as follows.

```{r joint_compare_param_est}
names(opt_out$par) <- names(out$par)
true_params <- with(dat$params, c(
  gamma, B, cov_to_theta(Psi), cov_to_theta(sigma),
  delta, omega, alpha))
n_params <- length(true_params)
names(true_params) <- names(out$par)[seq_along(true_params)]
rbind(Estimate = opt_out$par[1:n_params], 
      `True value` = true_params)
```

Next, we compare the estimated covariance matrix of the random effects with 
the true values.

```{r joint_show_psi}
# random effect covariance matrix (first estimated and then the true values)
is_psi <- which(grepl("Psi", names(true_params)))
theta_to_cov(opt_out$par[is_psi]) 
dat$params$Psi
cov2cor(theta_to_cov(opt_out$par[is_psi]))
cov2cor(dat$params$Psi)
```

Further, we compare the estimated covariance matrix of the noise with 
the true values.

```{r joint_show_sigma}
# noise covariance matrix (first estimated and then the true values)
is_sigma <- which(grepl("Sigma", names(true_params)))
theta_to_cov(opt_out$par[is_sigma])
dat$params$sigma
cov2cor(theta_to_cov(opt_out$par[is_sigma]))
cov2cor(dat$params$sigma)
```

We can look at quantiles of mean, standard deviations, and 
Pearson's moment coefficient of skewness for each individuals
estimated variational distribution as follows.

```{r joint_check_va}
va_stats <- lapply(1:1000, function(id){
  is_grp_x <- which(grepl(paste0("^g", id, ":"), names(opt_out$par)))
  x_va_pars <- opt_out$par[is_grp_x]
  xi <- x_va_pars[grepl(":xi", names(x_va_pars))]
  Lambda <- theta_to_cov(
    x_va_pars[grepl(":(log_sd|L)", names(x_va_pars))])
  alpha <- x_va_pars[grepl(":alpha", names(x_va_pars))]
  
  dp_to_cp(xi = xi, Psi = Lambda, alpha = alpha)
})

sum_func <- function(x)
  apply(x, 2L, quantile, probs = seq(0, 1, by = .1))

# mean 
sum_func(do.call(rbind, lapply(va_stats, `[[`, "mu")))

# standard deviation
sum_func(do.call(rbind, lapply(va_stats, 
                               function(x) sqrt(diag(x[["Sigma"]])))))

# skewness
skews <-  sum_func(do.call(rbind, lapply(va_stats, `[[`, "gamma")))
skews[] <- sprintf("%8.4f", skews)
print(skews, quote = FALSE)
```

We only see a low amount of skewness.

```{r joint_cleanup, echo = FALSE, results = "hide"}
rm(list = ls())
gc()
clear_cppad_mem(6L)
```


## Pedigree Data

<!-- $$\begin{align*}  -->
<!-- g(S(t\mid \vec x_{ij}, \epsilon_{ij})) &=  -->
<!--   \vec\omega^\top\vec f(t) + \vec\beta^\top\vec x_{ij} +\epsilon_{ij} \\ -->
<!-- \vec\epsilon_i &= (\epsilon_{i1}, \dots, \epsilon_{in_i})^\top \sim  -->
<!--   N^{(n_i)}\left(\vec 0, \sum_{l = 1}^K\sigma_l^2 C_{il} -->
<!--   \right) -->
<!-- \end{align*}$$ -->

The package contains an implementation of models which can used to estimate 
heritability using pedigree data. These are GSMs of the following form 

$$\begin{align*} g(S(t\mid \vec x_{ij}, \epsilon_{ij})) &=   \vec\omega^\top\vec f(t) + \vec\beta^\top\vec x_{ij} +\epsilon_{ij} \\\vec\epsilon_i &= (\epsilon_{i1}, \dots, \epsilon_{in_i})^\top \sim   N^{(n_i)}\left(\vec 0, \sum_{l = 1}^K\sigma_l^2 C_{il}  \right)\end{align*}$$

where $g$ is a given link function, $\vec f$ is a given function, the 
$\epsilon_{ij}$s are individual specific random effects, and the $K$ 
$C_{il}$ matrices are known. Various types of $C_{il}$ matrices can be used. 
A typical example is to use a kinship matrix to estimate genetic effect 
Other examples are to include 
maternal effects, paternal effects, shared environment etc. 

As an example, we will use the `pedigree.RDS` in the 
[inst/test-data](inst/test-data) directory. 

<!-- knitr::opts_knit$set(output.dir = ".") -->
<!-- knitr::load_cache("README-pedigree_example", path = "cache/") -->

```{r pedigree_example, cache = 1}
# load the data
library(survTMB)
dat <- readRDS(file.path("inst", "test-data", "pedigree.RDS"))
  
# prepare the cluster data
c_data <- lapply(dat$sim_data, function(x){
  data <- data.frame(Z = x$Z, y = x$y, event = x$event)
  cor_mats <- list(x$rel_mat, x$met_mat)
  list(data = data, cor_mats = cor_mats)
})

# the data structue is as in Mahjani et al. (2020). As an example, the full 
# family for the first family is shown here
library(kinship2)
plot(dat$sim_data[[1L]]$pedAll)

# we only have the "last row" (youngest generation). There are two effects 
# like in Mahjani et al. (2020). See table S17 in supplemental information. 
# The first is a genetic effects which correlation matrix is as follows
par(mar = c(2, 2, 1, 1))
cl <- colorRampPalette(c("Red", "White", "Blue"))(101)
rev_img <- function(x, ...)
  image(x[, NROW(x):1], ...)
rev_img(dat$sim_data[[1L]]$rel_mat, xaxt = "n", yaxt = "n", col = cl, 
        zlim = c(-1, 1))
library(Matrix)
as(dat$sim_data[[1L]]$rel_mat[1:8, 1:8], "sparseMatrix")

# secondly there is a maternal effect which correlation matrix is as follows
rev_img(dat$sim_data[[1L]]$met_mat, xaxt = "n", yaxt = "n", col = cl, 
        zlim = c(-1, 1))
as(dat$sim_data[[1L]]$met_mat[1:8, 1:8], "sparseMatrix")

# some summary stats are
length(c_data)    # number of clusters (families)

# number of obs / cluster
obs_in_cl <- sapply(c_data, function(x) NCOL(x$cor_mats[[1L]])) 
table(obs_in_cl) # distribution of cluster sizes

# total number of observations
sum(obs_in_cl)

# number of observed events
sum(sapply(c_data, function(x) sum(x$data$event)))

# use a third order polynomial as in the true model
sbase_haz <- function(x){
  x <- log(x)
  cbind(x^3, x^2, x)
}
dsbase_haz <- function(x){
  y <- log(x)
  cbind(3 * y^2, 2 * y, 1) / x
}

# create ADFun
system.time(
  func <- make_pedigree_ADFun(
    formula = Surv(y, event) ~ Z.1 + Z.2 - 1, 
    tformula  = ~  sbase_haz(y) - 1, trace = TRUE, 
    dtformula = ~ dsbase_haz(y) - 1,
    c_data = c_data, link = "probit", n_threads = 6L))

-func$fn(func$par) # lower bound of the log-likelihood

# check memory usage
# TODO: export the function
siz <- survTMB:::pedigree_get_size(environment(func$fn)$adfun)

# the amount of work and memory necessary for computing function values 
# and derivatives using f is roughly proportional to ...
siz$size_var 
```

```{r pedigree_optim, cache = 1}
# optimize and compare the results with the true parameters. We start by 
# using a GVA
library(lbfgs)

# quite ad-hock solution to convergence issues
func_start_val <- func$fn(func$par)
INF <- func_start_val + 1e5 * max(1, abs(func_start_val))
fn_wrapper <- function(par, ..., do_print = FALSE){
  out <- func$fn(par)
  if(is.nan(out))
    out <- INF
  
  if(do_print){
    model_pars <- which(!grepl("^g\\d+:", names(func$par)))
    cat(sprintf("%14.4f", par[model_pars]), "\n", sep = " ")
  }
  
  out
}
      
system.time(
  opt_out <- lbfgs(
    fn_wrapper, func$gr, func$par, m = 20, max_iterations = 25000L, 
    invisible = 1, delta = .Machine$double.eps^(1/3), past = 100L, 
    linesearch_algorithm = "LBFGS_LINESEARCH_BACKTRACKING_WOLFE", 
    max_linesearch = 20))
```

We show the estimates below and compare them with the true values.

```{r pedigree_show_est}
-opt_out$value # lower bound on the marginal log-likelihood in the end

rbind(
  `Starting values` = head(func$par, 7), 
  Estimates = head(opt_out$par, 7),
  `True values` = c(dat$omega, dat$beta, log(dat$sds)))

# check the skew parameters
names(opt_out$par) <- names(func$par)
reg_exp <- "(^g\\d+)(:.+$)"
va_ests <- opt_out$par[grepl(reg_exp, names(func$par), perl = TRUE)]
grp <- gsub(reg_exp, "\\1", names(va_ests), perl = TRUE)
cps <- tapply(va_ests, grp, function(x){
  n <- length(x)
  n <- .5 * (sqrt(8 * n + 25) - 5)
  dp_to_cp(xi = head(x, n), 
           Psi = theta_to_cov(tail(head(x, -n), -n)), 
           alpha = tail(x, n))
})

# distribution of skew parameters
summary(unlist(lapply(cps, `[[`, "gamma")))

# distribution of approximate means
summary(unlist(lapply(cps, `[[`, "mu")))

# example of some of the conditional covariance matrices
vcovs_ests <- lapply(cps[1:4 + 4], "[[", "Sigma")
par(mfcol = c(2, 2), mar = c(2, 2, 1, 1))
cl <- colorRampPalette(c("Red", "White", "Blue"))(201)
for(S in vcovs_ests)
  image(cor(S), xaxt = "n", yaxt = "n", col = cl, zlim = c(-1, 1))

# distribution of correlation matrix indices
sapply(vcovs_ests, function(x) {
  n <- NCOL(x)
  x <- cor(x)
  quantile(x[-cumsum(c(1L, rep(n + 1L, n - 1L)))])
}) 

# compare estimated covariance matrix with the true one for some of the 
# clusters
par(mfcol = c(2, 2), mar = c(2, 2, 4, 1))
local({
  is_sds <- grepl("^log_sds", names(func$par))
  sds_ests <- exp(opt_out$par[is_sds])
  sds_true <- dat$sds
  
  for(i in 1:4){
    cmats <- c_data[[i]]$cor_mats
    n_obs <- NCOL(cmats[[1L]])
    sig_est <- sig_tru <- diag(n_obs)
    for(m in seq_along(cmats)){
      sig_est <- sig_est + sds_ests[m]^2 * cmats[[m]]
      sig_tru <- sig_tru + sds_true[m]^2 * cmats[[m]]
    }
    
    mx <- max(abs(sig_est), abs(sig_tru))
    image(sig_est, xaxt = "n", yaxt = "n", col = cl, zlim = c(-mx, mx), 
          main = sprintf("Estimate (group %d)", i))
    image(sig_tru, xaxt = "n", yaxt = "n", col = cl, zlim = c(-mx, mx), 
          main = sprintf("Truth (group %d)", i))
  }
  
  invisible()
})
```

```{r pedigree_opt_ite, cache = 1}
# start at the true parameters
system.time(
  func <- make_pedigree_ADFun(
    formula = Surv(y, event) ~ Z.1 + Z.2 - 1, 
    omega = dat$omega, beta = dat$beta, sds = dat$sds,
    tformula  = ~  sbase_haz(y) - 1, trace = TRUE, 
    dtformula = ~ dsbase_haz(y) - 1,
    c_data = c_data, link = "probit", n_threads = 6L))

-func$fn(func$par) # lower bound of the log-likelihood

#####
# recursively update VA and model parameters
# returns a list with function which only requires some of the parameters
get_func <- function(x, keep){
  vx <- x
  vkeep <- keep
  list(
    fn = function(par, ...){
      vx[vkeep] <- par
      func$fn(vx)
    }, gr = function(par, ...){
      vx[vkeep] <- par
      func$gr(vx)[vkeep]
    })
}

# optimizer function
library(lbfgs)
foptim <- function(par, fn, gr, reltol, maxit = 100){
  if(length(par) > 1000L){
    return(lbfgs(
      call_eval = fn, call_grad = gr, vars = par, invisible = 1,
      m = 6L, epsilon = 1e-5, delta = reltol, past = 10L,
      max_iterations = maxit, max_linesearch = 30L,
      linesearch_algorithm = "LBFGS_LINESEARCH_BACKTRACKING_WOLFE"))
  }
  
  optim(par, fn, gr, control = list(reltol = reltol, maxit = maxit), 
        method = "BFGS")
  
}

# swap between updating VA parameters and model parameters
val <- func$par
is_va <- grepl("^g\\d+:", names(val))
is_mod <- which(!is_va)
is_va <- which(is_va)
reltol <- sqrt(.Machine$double.eps)

for(i in 1:100){
  opt_func_va <- get_func(val, is_va)
  new_res <- foptim(
    val[is_va], opt_func_va$fn, opt_func_va$gr, reltol = reltol)
  val[is_va] <- new_res$par
  
  opt_func_mod <- get_func(val, is_mod)
  new_res <- foptim(
    val[is_mod], opt_func_mod$fn, opt_func_mod$gr, reltol = reltol)
  val[is_mod] <- new_res$par
  
  cat(sprintf("Iteration %4d: %f\n", i, new_res$value))
  print(rbind(`True values` = c(dat$omega, dat$beta, log(dat$sds)), 
              Estimates = val[is_mod]))
}
```

## References
