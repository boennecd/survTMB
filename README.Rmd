---
output: 
  github_document: 
    pandoc_args: --webtex=https://latex.codecogs.com/svg.latex?
bibliography: README.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  cache.path = "cache/README-",
  out.width = "100%", error = FALSE)
options(digits = 3)
```

# survTMB
[![Build Status on Travis](https://travis-ci.org/boennecd/survTMB.svg?branch=master,osx)](https://travis-ci.org/boennecd/survTMB)
<!-- [![](https://www.r-pkg.org/badges/version/survTMB)](https://www.r-pkg.org/badges/version/survTMB) -->
<!-- [![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/survTMB)](https://cran.r-project.org/package=survTMB) -->

This package contains methods to estimated mixed generalized survival 
models [@Liu16;@Liu17]. All methods use automatic differentiation using 
the CppAD library [@Bell19] through [the TMB package](https://github.com/kaskr/adcomp) 
[@Kristensen16]. The estimation methods are 

 - a Laplace approximation using [TMB](https://github.com/kaskr/adcomp). 
 - Gaussian variational approximation (GVA) similar to the method shown 
   by @Ormerod12. 
 - Skew-normal variational approximation (SNVA) similar to the method shown
   by @Ormerod11.
   
The [example](#example) section shows an example of how to use the package
with different methods. The [benchmark](#benchmark) section shows a 
comparison of the computation time of the methods.

Joint marker and survival models are also available in the package. We show 
an example of estimating a joint model in the 
[joint models](#joint-models) section.

## Example

We estimate a GSM a below with the proportional odds (PO) link function 
using both a Laplace approximation, a GVA, and a SNVA. 
First, we define a function to perform the estimation.

```{r fit_example, cache = 1, fig.width = 5, fig.height = 3.67}
# assign variable with data 
dat <- coxme::eortc

# assign function to estimate the model
library(survTMB)
library(survival)
fit_model <- function(link, n_threads = 2L, method = "Laplace", 
                      param_type = "DP", dense_hess = FALSE, 
                      sparse_hess = FALSE, do_fit = TRUE){
  eval(bquote({
    adfun <- make_mgsm_ADFun(
      Surv(y, uncens) ~ trt, cluster = as.factor(center), 
      Z = ~ trt, df = 3L, data = dat, link = .(link), do_setup = .(method), 
      n_threads = .(n_threads), param_type = .(param_type), n_nodes = 15L, 
      dense_hess = .(dense_hess), sparse_hess = .(sparse_hess))
    fit <- if(.(do_fit))
      fit_mgsm(adfun, method = .(method)) else NULL
    list(fit = fit, fun = adfun)
  }), parent.frame())
}

# estimate the model using different methods. Start w/ Laplace
(lap_ph <- fit_model("PO"))$fit

# w/ GVA
(gva_fit <- fit_model("PO", method = "GVA"))$fit

# w/ SNVA
fit_model("PO", method = "SNVA", param_type = "DP")$fit
```

### Computing the Hessian

The Hessian using a variational approximation (VA) can be computed as both 
a dense and as sparse matrix. We show an example below where we compare the
two approaches. 

```{r load_micro}
library(microbenchmark) # needed for benchmarking
```

```{r comp_hess, cache = 1, dependson = "fit_example"}
# fit model w/ GVA
fit <- fit_model("PO", method = "GVA", dense_hess = TRUE, 
                 sparse_hess = TRUE)

# compute dense Hessian
par <- with(fit$fit, c(params, va_params))
dense_hess <- fit$fun$gva$he(par)

# has many zeros (i.e. it is sparse)
mean(abs(dense_hess) > 0) # fraction of non-zeros

# plot non-zero entries (black block's are non-zero; ignore upper triangle)
par(mar = c(1, 1, 1, 1))
is_non_zero <- t(abs(dense_hess) > 0)
is_non_zero[upper.tri(is_non_zero)] <- FALSE
image(is_non_zero, xaxt = "n", yaxt = "n", 
      col = gray.colors(2, 1, 0))

# compute sparse Hessian
sparse_hess <- fit$fun$gva$he_sp(par)

# they are identical 
stopifnot(isTRUE(
  all.equal(as.matrix(sparse_hess), dense_hess, check.attributes = FALSE)))

# compare storage cost
as.numeric(object.size(dense_hess) / object.size(sparse_hess))

# we usually want the first part the inverse negative Hessian for the model 
# parameters. This can be computed as follows
library(Matrix)
n_vars <- length(fit$fit$params)
naiv_vcov <- function(hess)
  solve(hess)[1:n_vars, 1:n_vars]
alte_vcov <- function(hess){
  idx <- 1:n_vars
  A <- hess[ idx,  idx]
  C <- hess[-idx,  idx]
  D <- hess[-idx, -idx]
  solve(A - crossprod(C, solve(D, C)))
}

# these are the asymptotic standard deviations
structure(sqrt(diag(alte_vcov(dense_hess))), names = names(fit$fit$params))

# check output is the same
stopifnot(
  isTRUE(all.equal(naiv_vcov(dense_hess), alte_vcov(dense_hess))),
  isTRUE(all.equal(naiv_vcov(dense_hess), as.matrix(alte_vcov(sparse_hess)), 
                   check.attributes = FALSE)),
  isTRUE(all.equal(naiv_vcov(dense_hess), as.matrix(naiv_vcov(sparse_hess)), 
                   check.attributes = FALSE)))

# compare computation time
microbenchmark(
  `Compute dense Hessian`               = fit$fun$gva$he(par), 
  `Compute sparse Hessian`              = fit$fun$gva$he_sp(par), 
  `Invert dense Hessian (naive)`        = naiv_vcov(dense_hess), 
  `Invert sparse Hessian (naive)`       = naiv_vcov(sparse_hess),
  `Invert dense Hessian (alternative)`  = alte_vcov(dense_hess), 
  `Invert sparse Hessian (alternative)` = alte_vcov(sparse_hess),
  times = 10)
```

The sparse matrix only becomes more favorable for larger data sets
(that is, in terms of the number of clusters). However, 
[recording](https://www.coin-or.org/CppAD/Doc/independent.htm) takes some 
time and requires additional memory. We illustrate the additional time 
below. 

```{r extra_time, cache = 1, dependson = "fit_example"}
microbenchmark(
  `W/o Hessians     ` = fit_model("PO", method = "GVA", do_fit = FALSE), 
  `W/ dense Hessian ` = fit_model("PO", method = "GVA", do_fit = FALSE, 
                                  dense_hess = TRUE), 
  `W/ sparse Hessian` = fit_model("PO", method = "GVA", do_fit = FALSE, 
                                  sparse_hess = TRUE), 
  times = 10)
```

### Other link functions
We estimate the same model below with other link functions.

```{r other_link, cache = 1, dependson = "fit_example"}
######
# w/ Laplace
fit_model("PH"    )$fit
fit_model("PO"    )$fit
fit_model("probit")$fit

######
# w/ GVA
fit_model("PH"    , method = "GVA")$fit
fit_model("PO"    , method = "GVA")$fit
fit_model("probit", method = "GVA")$fit

######
# w/ SNVA (DP: direct parameterization)
fit_model("PH"    , method = "SNVA", param_type = "DP")$fit
fit_model("PO"    , method = "SNVA", param_type = "DP")$fit
fit_model("probit", method = "SNVA", param_type = "DP")$fit

######
# w/ SNVA (CP: centralized parameterization)
fit_model("PH"    , method = "SNVA", param_type = "CP_trans")$fit
fit_model("PO"    , method = "SNVA", param_type = "CP_trans")$fit
fit_model("probit", method = "SNVA", param_type = "CP_trans")$fit
```

## Benchmark
We provide a benchmark of the estimation methods used in section 
[example](#example) below.

```{r comp_time, cache = 1, dependson = "fit_example"}
for(mth in c("Laplace", "GVA")){
  msg <- sprintf("Method: %s", mth)
  cat(sprintf("\n%s\n%s\n", msg, 
              paste0(rep("-", nchar(msg)), collapse = "")))
  print(microbenchmark(
    `PH         ` = fit_model("PH"    , 1L, mth),
    `PH     (2L)` = fit_model("PH"    , 2L, mth),
    `PH     (4L)` = fit_model("PH"    , 4L, mth),
    
    `PO         ` = fit_model("PO"    , 1L, mth),
    `PO     (2L)` = fit_model("PO"    , 2L, mth),
    `PO     (4L)` = fit_model("PO"    , 4L, mth), 
    
    `probit     ` = fit_model("probit", 1L, mth),
    `probit (2L)` = fit_model("probit", 2L, mth),
    `probit (4L)` = fit_model("probit", 4L, mth),
    times = 5))
}
```

```{r SNVA_comp_time, cache = 1, dependson = "fit_example"}
for(param_type in c("DP", "CP_trans")){
  mth <- "SNVA"
  msg <- sprintf("Method: %s (%s)", mth, param_type)
  cat(sprintf("\n%s\n%s\n", msg, 
              paste0(rep("-", nchar(msg)), collapse = "")))
  print(microbenchmark(
    `PH         ` = fit_model("PH"    , 1L, mth, param_type = param_type),
    `PH     (2L)` = fit_model("PH"    , 2L, mth, param_type = param_type),
    `PH     (4L)` = fit_model("PH"    , 4L, mth, param_type = param_type),
    
    `PO         ` = fit_model("PO"    , 1L, mth, param_type = param_type),
    `PO     (2L)` = fit_model("PO"    , 2L, mth, param_type = param_type),
    `PO     (4L)` = fit_model("PO"    , 4L, mth, param_type = param_type), 
    
    `probit     ` = fit_model("probit", 1L, mth, param_type = param_type),
    `probit (2L)` = fit_model("probit", 2L, mth, param_type = param_type),
    `probit (4L)` = fit_model("probit", 4L, mth, param_type = param_type),
    times = 5))
}
```

## Joint Models
We will use one of the test data sets in the 
[inst/test-data](inst/test-data) directory. The data is 
generated with the 
[inst/test-data/gen-test-data.R](inst/test-data/gen-test-data.R) file which 
is available on Github. The file uses the 
[SimSurvNMarker](https://github.com/boennecd/SimSurvNMarker) package 
to simulate a data set. The model is simulated from 

<!-- $$\begin{align*} -->
<!-- \vec Y_{ij} \mid \vec U_i = \vec u_i -->
<!--   &\sim N^{(r)}(\vec \mu_i(s_{ij}, \vec u_i), \Sigma) -->
<!--   \\ -->
<!-- \vec\mu(s, \vec u) &= -->
<!--   \Gamma^\top \vec x_i + B^\top\vec g(s) + U^\top\vec m(s) -->
<!--   \\ -->
<!-- &= \left(I \otimes \vec x_i^\top\right)\text{vec}\Gamma -->
<!--      + \left(I \otimes \vec g(s)^\top\right)\text{vec} B -->
<!--      + \left(I \otimes \vec m(s)^\top\right) \vec u -->
<!--   \\ -->
<!-- \vec U_i &\sim N^{(K)}(\vec 0, \Psi) -->
<!--   \\ -->
<!-- h(t\mid \vec u) &= \exp\left( -->
<!--   \vec\omega^\top\vec b(t) + -->
<!--   \vec z_i^\top\vec\delta + -->
<!--   \vec\alpha^\top\vec\mu(t, \vec u) -->
<!--   \right) -->
<!--   \\ -->
<!-- &= \exp\Bigg( -->
<!--   \vec\omega^\top\vec b(t) + -->
<!--   \vec z_i^\top\vec\delta -->
<!--   + \vec 1^\top\left( -->
<!--   \text{diag}(\vec \alpha) \otimes \vec x_i^\top\right)\text{vec}\Gamma -->
<!--   + \vec 1^\top\left( -->
<!--   \text{diag}(\vec \alpha) \otimes \vec g(t)^\top\right)\text{vec} B \\ -->
<!-- &\hspace{50pt}+ \vec 1^\top\left( -->
<!--   \text{diag}(\vec \alpha) \otimes \vec m(t)^\top\right)\vec u -->
<!--   \Bigg) -->
<!-- \end{align*}$$ -->

$$\begin{align*}  \vec Y_{ij} \mid \vec U_i = \vec u_i    &\sim N^{(r)}(\vec \mu_i(s_{ij}, \vec u_i), \Sigma)    \\  \vec\mu(s, \vec u) &=    \Gamma^\top \vec x_i + B^\top\vec g(s) + U^\top\vec m(s)    \\  &= \left(I \otimes \vec x_i^\top\right)\text{vec}\Gamma       + \left(I \otimes \vec g(s)^\top\right)\text{vec} B       + \left(I \otimes \vec m(s)^\top\right) \vec u    \\  \vec U_i &\sim N^{(K)}(\vec 0, \Psi)    \\  h(t\mid \vec u) &= \exp\left(    \vec\omega^\top\vec b(t) +    \vec z_i^\top\vec\delta +    \vec\alpha^\top\vec\mu(t, \vec u)    \right)    \\  &= \exp\Bigg(    \vec\omega^\top\vec b(t) +    \vec z_i^\top\vec\delta    + \vec 1^\top\left(    \text{diag}(\vec \alpha) \otimes \vec x_i^\top\right)\text{vec}\Gamma    + \vec 1^\top\left(    \text{diag}(\vec \alpha) \otimes \vec g(t)^\top\right)\text{vec} B \\  &\hspace{50pt}+ \vec 1^\top\left(    \text{diag}(\vec \alpha) \otimes \vec m(t)^\top\right)\vec u    \Bigg)  \end{align*}$$

where $\vec Y_{ij}\in\mathbb R^{n_y}$ is individual $i$'s 
$j$th observed marker at time $s_{ij}$, 
$U_i\in\mathbb R^K$ is individual $i$'s random effect, and 
$h$ is the instantaneous hazard rate for the time-to-event outcome. 
$\vec\alpha$ is the so-called
association parameter. It shows the strength of the relation between the 
latent mean function, $\vec\mu(t,\vec u)$, and the log of the 
instantaneous rate, $h(t\mid \vec u)$. $\vec m(t)$, $\vec g(t)$ 
and $\vec b(t)$ are 
basis expansions of time. As an example, these can be a polynomial, 
a B-spline, or a natural cubic spline. The expansion for the baseline
hazard, $\vec b(t)$, is typically made on $\log t$ instead of $t$. 
One reason is that the model reduces to a Weibull distribution
when a first polynomial is used and $\vec\alpha = \vec 0$. $\vec x_i$ and 
$\vec z_i$ are individual specific known covariates.

We start by loading the simulated data set.

```{r read_joint}
dat <- readRDS(file.path("inst", "test-data", "large-joint-all.RDS"))

# the marker data
m_data <- dat$marker_data
head(m_data, 10)

# the survival data
s_data <- dat$survival_data
head(s_data, 10)
```

There is 

```{r joint_show_n_ids}
length(unique(s_data$id))
length(unique(s_data$id)) == NROW(s_data) # one row per id
```

individuals who each has an average of 

```{r joint_avg_markers}
NROW(m_data) / length(unique(s_data$id))
```

observed markers. The data is simulated. Thus, we know the true parameters. 
These are

```{r joint_show_true_params}
dat$params[c("gamma", "B", "Psi", "omega", "delta", "alpha", "sigma")]
```

We start by constructing the objective function in order to estimate 
the model. 

```{r joint_get_ptr, cache = 1}
system.time(
  out <- make_joint_ADFun(
    sformula =  Surv(left_trunc, y, event) ~ Z1 + Z2, 
    mformula = cbind(Y1, Y2) ~ X1, 
    id_var = id, time_var = obs_time, 
    sdata = s_data, mdata = m_data, mknots = dat$params$m_attr$knots,
    sknots = dat$params$b_attr$knots, gknots = dat$params$g_attr$knots, 
    n_nodes = 30L, n_threads = 6L))
```

Next, we fit the model using the `lbfgs` function from the `lbfgs` package 
using the non-exported `.opt_default` function.

```{r joint_fit, cache = 1, dependson = "joint_get_ptr"}
system.time(
  opt_out <- survTMB:::.opt_default(
    out$par, out$fn, out$gr, control = list(maxit = 10000L)))
```

The estimated lower bound of the log marginal likelihood at the optimum is
shown below.

<!-- with(environment(out$fn), c(mark$ll, sr_dat$ll, mark$ll + sr_dat$ll)) -->

```{r joint_show_lb}
-opt_out$value
```

Further, we can compare the estimated model parameters with the true 
model parameters as follows.

```{r joint_compare_param_est}
names(opt_out$par) <- names(out$par)
true_params <- with(dat$params, c(
  gamma, B, survTMB:::.cov_to_theta(Psi), survTMB:::.cov_to_theta(sigma),
  delta, omega, alpha))
n_params <- length(true_params)
names(true_params) <- names(out$par)[seq_along(true_params)]
rbind(Estimate = opt_out$par[1:n_params], 
      `True value` = true_params)
```

Next, we compare the estimated covariance matrix of the random effects with 
the true values.

```{r joint_show_psi}
# random effect covariance matrix (first estimated and then the true values)
is_psi <- which(grepl("Psi", names(true_params)))
survTMB:::.theta_to_cov(opt_out$par[is_psi]) 
dat$params$Psi
cov2cor(survTMB:::.theta_to_cov(opt_out$par[is_psi]))
cov2cor(dat$params$Psi)
```

Further, we compare the estimated covariance matrix of the noise with 
the true values.

```{r joint_show_sigma}
# noise covariance matrix (first estimated and then the true values)
is_sigma <- which(grepl("Sigma", names(true_params)))
survTMB:::.theta_to_cov(opt_out$par[is_sigma])
dat$params$sigma
cov2cor(survTMB:::.theta_to_cov(opt_out$par[is_sigma]))
cov2cor(dat$params$sigma)
```

```{r check_va, echo = FALSE, eval = FALSE}
is_grp_x <- which(grepl("^g2:", names(opt_out$par)))
x_va_pars <- opt_out$par[is_grp_x]
xi <- x_va_pars[grepl(":xi", names(x_va_pars))]
Lambda <- survTMB:::.theta_to_cov(
  x_va_pars[grepl(":(log_sd|L)", names(x_va_pars))])
rho <- x_va_pars[grepl(":rho", names(x_va_pars))]

k <- Lambda %*% rho
k <- drop(k / sqrt(drop(1 + rho %*% k)))
Lambda - 2 / pi * outer(k, k)
cov2cor(Lambda - 2 / pi * outer(k, k))
```

## References
